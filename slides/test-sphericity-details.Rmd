---
title: "Test for sphericity"
author: "Max Turgeon"
date: "03/02/2020"
output: pdf_document
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amsthm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We assume $\mathbf{Y}_1, \ldots, \mathbf{Y}_n \sim N_p(\mu, \Sigma)$ with $\Sigma$ positive definite. Write $V = n\hat{\Sigma}$, where $(\bar{\mathbf{Y}}, \hat{\Sigma})$ is the (unrestricted) MLE for the multivariate normal distribution.

*Sphericity* means the different components of $\mathbf{Y}$ are **uncorrelated** and have the **same variance**. In other words, we are looking at the following null hypothesis:
$$ H_0 : \Sigma = \sigma^2 I_p, \qquad \sigma^2 > 0.$$

## Likelihood Ratio Test

We have
\begin{align*}
L(\hat{\mathbf{Y}}, \sigma^2 I_p) &= (2\pi)^{-np/2}\lvert\sigma^2 I_p\rvert^{-n/2}\exp\left(-\frac{1}{2}\mathrm{tr}((\sigma^2 I_p)^{-1}V)\right)\\
  &= (2\pi\sigma^2)^{-np/2}\exp\left(-\frac{1}{2\sigma^2}\mathrm{tr}(V)\right).
\end{align*}
Taking the derivative of the logarithm and setting it equal to zero, we find that $L(\hat{\mathbf{Y}}, \sigma^2 I_p)$ is maximised when
$$\widehat{\sigma^2} = \frac{\mathrm{tr} V}{np}.$$
We then get
\begin{align*}
L(\hat{\mathbf{Y}}, \widehat{\sigma^2} I_p) &= (2\pi\widehat{\sigma^2})^{-np/2}\exp\left(-\frac{1}{2\widehat{\sigma^2}}\mathrm{tr}(V)\right)\\
  &= (2\pi)^{-np/2}\left(\frac{\mathrm{tr} V}{np}\right)^{-np/2}\exp\left(-\frac{np}{2}\right).
\end{align*}
Therefore, we have
\begin{align*}
\Lambda &= \frac{(2\pi)^{-np/2}\left(\frac{\mathrm{tr} V}{np}\right)^{-np/2}\exp\left(-\frac{np}{2}\right)}{\exp(-np/2)(2\pi)^{-np/2}\lvert\hat{\Sigma}\rvert^{-n/2}}\\
  &= \frac{\left(\frac{\mathrm{tr} V}{np}\right)^{-np/2}}{\lvert n^{-1}V\rvert^{-n/2}}\\
  &= \left(\frac{\lvert V\rvert}{(\mathrm{tr} V/p)^p}\right)^{n/2}.
\end{align*}

We can also rewrite this as follows: let $l_1 \geq \cdots \geq l_p$ be the eigenvalues of $V$. We have
\begin{align*}
\Lambda^{2/n} &= \frac{\lvert V\rvert}{(\mathrm{tr} V/p)^p}\\
  &= \frac{\prod_{j=1}^p l_j}{(\frac{1}{p}\sum_{j=1}^p l_j)^p}\\
  &= \left(\frac{\prod_{j=1}^p l_j^{1/p}}{\frac{1}{p}\sum_{j=1}^p l_j}\right)^p.
\end{align*}
In other words, the modified LRT $\tilde{\Lambda} = \Lambda^{2/n}$ is the ratio of the geometric to the arithmetic mean of the eigenvalues of $V$ (all raised to the power $p$).

Note that under $H_0$, there is only one free parameter, namely $\sigma^2$. Therefore the aymptotic theory of likelihood ratio tests implies that
$$-2 \log \Lambda \to \chi^2\left(\frac{1}{2}p(p+1) - 1\right).$$

We will provide a better approximation using asymptotic expansions. 

First, we need to compute the moments of $\Lambda$. We start with the following lemma:

### Lemma

Under the null hypothesis, the random variables $\mathrm{tr} V$ and $\frac{\det V}{(\mathrm{tr} V)^p}$ are independent.

*Proof*

Recall that $V \sim W_p(n-1, \sigma^2 I_p)$, and so its distribution only depends on $\sigma^2$. Hence, the distribution of $\frac{\det V}{(\mathrm{tr} V)^p}$ does *not* depend on $\sigma^2$, and therefore it is an ancillary statistic.

Now, given that $\widehat{\sigma^2} = \frac{\mathrm{tr} V}{np}$ and given that the multivariate normal forms an exponential family, we know that $(\bar{\mathbf{Y}}, \mathrm{tr} V)$ is a minimal sufficient and complete statistic. Therefore, we can conclude by using Basu's theorem. \hfill\qed

Now, going back to $\tilde{\Lambda}$, note that we have
$$ \tilde{\Lambda} \left(\frac{1}{p}\mathrm{tr}V\right)^p = \lvert V \rvert.$$
Using our lemma above, for any $h$, we can write
\begin{align*}
E \lvert V \rvert^h &= E\left(\tilde{\Lambda} \left(\frac{1}{p}\mathrm{tr}V\right)^p\right)^h \\
  &= E\tilde{\Lambda}^h E\left(\frac{1}{p}\mathrm{tr}V\right)^{ph}.
\end{align*}
In other words, we have
$$ E\left(\tilde{\Lambda}^h\right) = \frac{E \left(\lvert V \rvert^h\right)}{E\left(\left(\frac{1}{p}\mathrm{tr}V\right)^{ph}\right)}.$$

Recall the following two results:

  1. If $W \sim W_p(m, I_p)$, then $\mathrm{tr} W \sim \chi^2(mp)$.
  2. If $W \sim W_p(m, \Sigma)$, then $\lvert W \rvert \sim \lvert\Sigma\rvert \prod_{j=1}^p \chi^2(m - p + j)$.
  
Therefore, we can get all the moments of $\tilde{\Lambda}$ from the moments of chi-squared distributions. 

### Proposition

Let $X\sim\chi^2(d)$. Then for $h > -\frac{1}{2}d$, we have
$$ E(X^h) = 2^h \frac{\Gamma\left(\frac{1}{2}d + h\right)}{\Gamma\left(\frac{1}{2}d\right)}.$$

Putting all this together, we get the following theorem:

### Theorem

The moments of the modified LRT statistic are given by
$$ E\left(\tilde{\Lambda}^h\right) = p^{ph}\frac{\Gamma\left(\frac{1}{2}(n - 1)p\right)}{\Gamma\left(\frac{1}{2}(n - 1)p + ph\right)}\frac{\Gamma_p\left(\frac{1}{2}(n - 1) + h\right)}{\Gamma_p\left(\frac{1}{2}(n - 1)\right)}.$$

*Proof*

This follows from our discussion above, the moments of the chi-squared distribution, and the fact that $V \sim \sigma^2W_p(n-1)$.\hfill\qed


## Asymptotic expansion

In a 1949 *Biometrika* paper, George Box studied the distribution theory of a very general class of likelihood ratio tests. It can be applied any time the moments of the likelihood ratio $\Lambda$ (or some power $W = \Lambda^d$ thereof) have the following expression:

\begin{equation}\label{eqn:box} E\left(W^h\right) = K\left(\frac{\prod_{j=1}^by_j^{y_j}}{\prod_{k=1}^ax_k^{x_k}}\right)^h\frac{\prod_{k=1}^a\Gamma\left(x_k(1 + h) + \zeta_k\right)}{\prod_{j=1}^b\Gamma\left(y_j(1 + h) + \eta_j\right)},\end{equation}
such that 
$$ \sum_{j=1}^b y_j = \sum_{k=1}^a x_k$$
and $K$ is a constant such that $E\left(\tilde{\Lambda}^0\right) = 1$.

In the context of the test for sphericity, we can take $W = \Lambda^{m/n}$ with $m = n-1$, and we get
\begin{align*}
E\left(W^h\right) &= E\left(\Lambda^{mh/n}\right)\\ 
  &= E\left(\tilde{\Lambda}^{mh/2}\right)\\
  &= p^{pmh/2}\frac{\Gamma\left(\frac{1}{2}mp\right)}{\Gamma\left(\frac{1}{2}mp + \frac{1}{2}pmh\right)}\frac{\Gamma_p\left(\frac{1}{2}m + \frac{1}{2}mh\right)}{\Gamma_p\left(\frac{1}{2}m\right)}\\
  &= \left(\frac{\Gamma\left(\frac{1}{2}mp\right)}{\Gamma_p\left(\frac{1}{2}m\right)}\right)p^{pmh/2}\frac{\Gamma_p\left(\frac{1}{2}m(1 + h)\right)}{\Gamma\left(\frac{1}{2}mp(1 + h)\right)}\\
  &= \left(\pi^{p(p-1)/4}\frac{\Gamma\left(\frac{1}{2}mp\right)}{\Gamma_p\left(\frac{1}{2}m\right)}\right)\left(p^{pm/2}\right)^h\frac{\prod_{k=1}^p\Gamma_p\left(\frac{1}{2}m(1 + h) - \frac{1}{2}(k - 1)\right)}{\Gamma\left(\frac{1}{2}mp(1 + h)\right)}.
\end{align*}

This is consistent with the general form above, if we take
$$\begin{matrix}
a = p, & x_k = \frac{1}{2}m, & \zeta_k = -\frac{1}{2}(k-1),\\
b = 1, & y_1 = \frac{1}{2}mp, & \eta_1 = 0,
\end{matrix}$$
and
$$K = \pi^{p(p-1)/4}\frac{\Gamma\left(\frac{1}{2}mp\right)}{\Gamma_p\left(\frac{1}{2}m\right)}.$$
